{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import requests\n",
    "\n",
    "from urllib import parse\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_colwidth', 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_life = pd.read_csv('input/LIFE-projects.csv')\n",
    "all_life.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_life.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_life.rename(columns={ \n",
    "    all_life.columns[0]: \"project_websummary\",\n",
    "    all_life.columns[1]: \"project_title\",\n",
    "    all_life.columns[2]: \"project_no\",\n",
    "    all_life.columns[3]: \"project_website\",\n",
    "    all_life.columns[4]: \"year_of_finance\",\n",
    "    all_life.columns[5]: \"lead_partner_country\",\n",
    "    all_life.columns[6]: \"type_of_beneficiary\",\n",
    "    all_life.columns[7]: \"country\",\n",
    "    all_life.columns[8]: \"themes\",\n",
    "    all_life.columns[9]: \"keywords\",\n",
    "    all_life.columns[10]: \"habitats\",\n",
    "    all_life.columns[11]: \"species\"\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_life['base_url'] = 'http://ec.europa.eu/environment/life/project/Projects/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_life['project_title'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_life.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_life.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_life['project_url'] = all_life['base_url'] + all_life['project_websummary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_from_url(url, param_name):\n",
    "    return [i.split(\"=\")[-1] for i in url.split(\"?\", 1)[-1].split(\"&\") if i.startswith(param_name + \"=\")][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_life['project_id'] = all_life.apply(lambda row: get_param_from_url(row['project_url'], 'n_proj_id'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_life = all_life[all_life.country=='United Kingdom'].copy()\n",
    "# uk_life = uk_life.sort_values(by=['project_id'])\n",
    "uk_life.shape\n",
    "uk_life.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now need to examine web page to see if we attempt geolocation using Natura 2000 dataset. Project id 6699 has none, 6812 has a few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_life.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Natura sites from Life website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeNaturaSites(check_url):\n",
    "    natura_project = pd.DataFrame([], columns=[\"area_type\", \"area_code\", \"area_name\"])\n",
    "    raw_contents = urllib.request.urlopen(check_url)\n",
    "    charset=raw_contents.info().get_content_charset()\n",
    "    contents=raw_contents.read().decode(charset)\n",
    "    soup = BeautifulSoup(contents, 'html5lib')\n",
    "    # html tags to find \n",
    "    ''' <span class=\"txtheadergreen\">Natura 2000 sites</span> '''\n",
    "    natura_span=soup.find('span',string='Natura 2000 sites')\n",
    "    # html tags to find \n",
    "    ''' <table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\">\n",
    "            <tbody><tr><td valign=\"top\">SPA</td>\n",
    "            <td valign=\"top\">UK9010101</td>\n",
    "            <td valign=\"top\">Dorset Heathlands</td>\n",
    "            </tr> '''\n",
    "    natura_table1=natura_span.findNext('table')\n",
    "    # If the first table has 3 elements then it appears to be single row col containing 'Not applicable'\"\n",
    "    natura_first_td=natura_table1.findNext('td')\n",
    "    if natura_first_td.string == 'Not applicable':\n",
    "        print('No Natura Sites found')\n",
    "        return natura_project\n",
    "    # Convert html table to a dictionary\n",
    "    table_rows = natura_table1.find_all('tr')\n",
    "    print(table_rows.count)\n",
    "    res = []\n",
    "    for tr in table_rows:\n",
    "        td = tr.find_all('td')\n",
    "        row = [tr.text.strip() for tr in td if tr.text.strip()]\n",
    "        if row:\n",
    "            res.append(row)\n",
    "    print(type(res))\n",
    "    #natura_project = natura_project.append(res, ignore_index=True)\n",
    "    natura_project = pd.DataFrame(res, columns=[\"area_type\", \"area_code\", \"area_name\"])\n",
    "    return natura_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_postcodes = pd.DataFrame(columns=['project_id', 'area_name', 'postcode'])\n",
    "index = 0\n",
    "for index, row in uk_life.iterrows():\n",
    "    print('In loop, index is ' + str(index))\n",
    "    #if index > 50:\n",
    "    #    break\n",
    "    print(row['project_id'], row['project_title'])\n",
    "    # Should return dataframe containing \"area_type\", \"area_code\", \"area_name\" (if there are any Natura sites)\n",
    "    # natura_sites = downloadNaturaSites('http://ec.europa.eu/environment/life/project/Projects/index.cfm?fuseaction=search.dspPage&n_proj_id=6812')\n",
    "    natura_sites = scrapeNaturaSites(row['project_url'])\n",
    "    if natura_sites.empty:\n",
    "        print('No Natura sites for project_id ' + row['project_id'])\n",
    "    else:\n",
    "        print('Natura sites found for project_id ' + row['project_id'])\n",
    "        print(natura_sites)\n",
    "        natura_sites['project_id'] = row['project_id']\n",
    "        natura_sites['postcode'] = natura_sites.apply(lambda x: getPostcodeForNaturaSite(x['area_code']), axis=1)\n",
    "        project_postcodes = project_postcodes.append(natura_sites,ignore_index=True)\n",
    "project_postcodes.to_csv('output/projectpostcodes.csv', encoding='utf-8')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectNaturaSite = scrapeNaturaSites('http://ec.europa.eu/environment/life/project/Projects/index.cfm?fuseaction=search.dspPage&n_proj_id=6698')\n",
    "projectNaturaSite.head(2)                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: We will need to split the budget across this many areas. Do the other data sets do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Long and Lat for a specific Natura area code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natura site info can be downloaded from https://www.eea.europa.eu/data-and-maps/data/natura-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Stretching over 18 % of the EUâ€™s land area and almost 6 % of its marine territory, it is the largest coordinated network of protected areas in the world. It offers a haven to Europe's most valuable and threatened species and habitats.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPostcodeForNaturaSite(this_natura_area_code):\n",
    "    latAndLong = findLatAndLong(this_natura_area_code)\n",
    "    if latAndLong == [0,0]:\n",
    "        return 'N/A NoArea'\n",
    "    postcode = findNearestPostcode(latAndLong[0],latAndLong[1])\n",
    "    print('Postcode for Natura area code ' + this_natura_area_code + ' is ' + postcode)\n",
    "    return postcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it out. IE0004009 is an ROI code that's slipped in. This should return a N/A NoArea postcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_postcode = getPostcodeForNaturaSite('IE0004009')\n",
    "print(my_postcode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UK9005151 is Bowland Forest and should have East Lancs postcode beginning with BB7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_postcode = getPostcodeForNaturaSite('UK9005151')\n",
    "print(my_postcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findLatAndLong(natura_area_code):\n",
    "    print('Finding lag/long for area code ' + str(natura_area_code))\n",
    "    # http://ec.europa.eu/environment/life/project/Projects/index.cfm\n",
    "    all_natura_sites = pd.read_csv('input/NATURA2000SITES.csv')\n",
    "    uk_natura_sites = all_natura_sites[all_natura_sites.COUNTRY_CODE=='uk'].copy()\n",
    "    # Stip out spurious columns\n",
    "    uk_natura_sites_reduced = uk_natura_sites[['SITECODE', 'SITENAME', 'LONGITUDE','LATITUDE']].copy()\n",
    "    uk_natura_sites_reduced.rename(columns={ \n",
    "        uk_natura_sites_reduced.columns[0]: \"site_code\",\n",
    "        uk_natura_sites_reduced.columns[1]: \"site_name\",\n",
    "        uk_natura_sites_reduced.columns[2]: \"longitude\",\n",
    "        uk_natura_sites_reduced.columns[3]: \"latitude\"\n",
    "        }, inplace=True)\n",
    "    area_code_row=uk_natura_sites_reduced[uk_natura_sites_reduced.site_code==natura_area_code].copy()\n",
    "    if area_code_row.empty:\n",
    "        longitude = 0\n",
    "        latitude = 0\n",
    "    else:\n",
    "        longitude = area_code_row['longitude'].values[0]\n",
    "        latitude = area_code_row['latitude'].values[0]\n",
    "    return [latitude, longitude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findLatAndLong('UK0013027')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the nearest postcode for a lat/long using the marvellous postcodes.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findNearestPostcode(lat, long):\n",
    "    url = 'http://api.postcodes.io/postcodes'\n",
    "    data = '''{\n",
    "        \"geolocations\": [{\n",
    "        \"longitude\": ''' + str(long) + ''',\n",
    "        \"latitude\": ''' + str(lat) + ''',\n",
    "        \"radius\": 10,\n",
    "        \"limit\": 1,\n",
    "        \"wideSearch\" : true\n",
    "        }]\n",
    "    }'''\n",
    "    dump=json.dumps(data, indent=4, sort_keys=True)\n",
    "    # print(dump)\n",
    "    headers = {'Content-type': 'application/json', 'Accept': 'text/plain'}\n",
    "    response = requests.post(url, data=data, headers=headers)\n",
    "    # print(response)\n",
    "    if response.json()['result'][0]['result'] is None:\n",
    "        return 'N/A NoPostcode'\n",
    "    else:\n",
    "        # print('Result item value ' + response.json()['result'][0]['result'])\n",
    "        postcode = response.json()['result'][0]['result'][0]['postcode']\n",
    "        # print('Postcode is ' + postcode)\n",
    "        return postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findNearestPostcode(54.350556000000005,-3.429722)\n",
    "#findNearestPostcode(54.119167000000004,-2.961667)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read this back in from disk as the main loop is fairly inefficient currently and takes 60 sec or so. Also not sure what the throttling is on postcodes.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcode_to_project = pd.read_csv('output/projectpostcodes.csv', encoding='utf-8')\n",
    "postcode_to_project.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcode_to_project[\"project_id\"] = pd.to_numeric(postcode_to_project[\"project_id\"])\n",
    "uk_life[\"project_id\"] = pd.to_numeric(uk_life[\"project_id\"])\n",
    "uk_life_with_postcodes = pd.merge(uk_life, postcode_to_project, on='project_id', how='left')\n",
    "uk_life_with_postcodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukpostcodes = pd.read_csv('../postcodes/input/ukpostcodes.csv.gz')\n",
    "ukpostcodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_life_with_postcodes.postcode.isin(ukpostcodes.postcode).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_life_with_postcodes['clean_postcode'] = uk_life_with_postcodes.postcode.\\\n",
    "    str.upper().\\\n",
    "    str.strip().\\\n",
    "    str.replace(r'[^A-Z0-9]', '').\\\n",
    "    str.replace(r'^(\\S+)([0-9][A-Z]{2})$', r'\\1 \\2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_life_with_postcodes.postcode[~uk_life_with_postcodes.clean_postcode.isin(ukpostcodes.postcode)].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukpostcodes.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets simply drop all life rows without a valid postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_life = pd.merge(uk_life_with_postcodes, ukpostcodes, on='postcode', how='inner')\n",
    "clean_life.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape project info not in xls download from the project details web page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately not all the information we need is in the downloadable xls so we'll need to go to the project url and parse the html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeProjectDetails(project_id, check_url):\n",
    "    # Not much useful structure in the project webpages so just simple span search looking for strings\n",
    "    import re\n",
    "    raw_contents = urllib.request.urlopen(check_url)\n",
    "    charset = raw_contents.info().get_content_charset()\n",
    "    contents = raw_contents.read().decode(charset)\n",
    "    soup = BeautifulSoup(contents, 'html5lib')\n",
    "    # Get the beneficiary organisation (coordinator)\n",
    "    ''' <span class=\"txtheadergreen\">Coordinator</span> '''\n",
    "    coordinator_span = soup.find('span',string='Coordinator')\n",
    "    ''' <td width=\"60%\" align=\"left\">Natural England</td> '''\n",
    "    coordinator = coordinator_span.findNext('td').string\n",
    "    # Then get the total budget\n",
    "    # html tags to find \n",
    "    ''' <span class=\"txtheadergreen\">Total budget</span> '''\n",
    "    life_total_budget_span = soup.find('span',string='Total budget')\n",
    "    # html tags to find \n",
    "    ''' <td width=\"60%\" align=\"left\">8,522,712.00&nbsp;â‚¬</td> '''\n",
    "    raw_total_budget = life_total_budget_span.findNext('td').string\n",
    "    total_budget = re.sub('[^0-9.]', '', raw_total_budget)\n",
    "    #total_budget = int(stripNonNumeric(raw_total_budget ))\n",
    "    # Then get the EU contribution\n",
    "    # html tags to find \n",
    "    ''' <span class=\"txtheadergreen\">EU contribution</span> '''\n",
    "    life_eu_contribution_span = soup.find('span',string='EU contribution')\n",
    "    # html tags to find \n",
    "    ''' <td width=\"60%\" align=\"left\">5,113,627.00&nbsp;â‚¬</td> '''\n",
    "    raw_eu_contribution=life_eu_contribution_span.findNext('td').string\n",
    "    eu_contribution = re.sub('[^0-9.]', '', raw_eu_contribution)\n",
    "    # Then get the project background blurb\n",
    "    ''' <span class=\"txtheadergreen\">Background</span> '''\n",
    "    life_background_span = soup.find('span',string='Background')\n",
    "    ''' <p>    A decline in the quality of sand dune habitats ...   </p>'''\n",
    "    # Text within a <p> can contain tags like <i>. This feels a bit clunky.\n",
    "    background = str(life_background_span.findNext('p'))\n",
    "    #background = re.sub('[<p>]', '', background)\n",
    "    #background = re.sub('[</p>]', '', background)\n",
    "    if background is not None:\n",
    "        # Remove the paragraph tags then we can strip out the tabs and new lines\n",
    "        background = background.replace('<p>','')\n",
    "        background = background.replace('</p>','')\n",
    "        background = background.strip()\n",
    "    else:\n",
    "        background = ''\n",
    "    dictProjDetails = {}\n",
    "    dictProjDetails['project_id'] = project_id\n",
    "    dictProjDetails['coordinator'] = coordinator\n",
    "    dictProjDetails['total_budget'] = total_budget\n",
    "    dictProjDetails['eu_contribution'] = eu_contribution\n",
    "    dictProjDetails['background'] = background\n",
    "    print( [coordinator, total_budget, eu_contribution, background[:25] + '...'])\n",
    "    return dictProjDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_details = pd.DataFrame(columns=['project_id', 'coordinator', 'total_budget', 'eu_contribution', 'background'])\n",
    "for index, row in uk_life.iterrows():\n",
    "    dictProjectDetails = scrapeProjectDetails(row['project_id'], row['project_url'])\n",
    "    project_details = project_details.append(dictProjectDetails,ignore_index=True)                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_details.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_details.to_csv('output/projectdetails.csv', encoding='utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_details = pd.read_csv('output/projectdetails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_details[\"project_id\"] = pd.to_numeric(project_details[\"project_id\"])\n",
    "clean_life = pd.merge(clean_life, project_details, on='project_id', how='inner')\n",
    "clean_life.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_life.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#life_budgets = scrapeProjectDetails('http://ec.europa.eu/environment/life/project/Projects/index.cfm?fuseaction=search.dspPage&n_proj_id=5344')\n",
    "#print(life_budgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_life.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_life.to_csv('output/cleanlife.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to GBP\n",
    "\n",
    "All we have is a year in life_clean, so just use the average annual exchange rate. Project range is specified in the project website page but we will ingore that for now. This code taken from FTS ingest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eur_gbp = pd.read_pickle('../exchange_rates/output/exchange_rates.pkl.gz')\n",
    "eur_gbp.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_average_eur_gbp_rate():\n",
    "    # create timeseries from start to end\n",
    "    days = pd.date_range('2016-01-01', '2017-01-01', closed='left')\n",
    "    daily = pd.DataFrame({\n",
    "        'month_start': days,\n",
    "        'weight': 1.0 / days.shape[0]\n",
    "    })\n",
    "    monthly = daily.resample('MS', on='month_start').sum()\n",
    "    monthly = pd.merge(monthly, eur_gbp, on='month_start', validate='1:1')\n",
    "    return (monthly.weight * monthly.rate).sum()\n",
    "clean_life['eur_gbp'] = find_average_eur_gbp_rate()\n",
    "clean_life.eur_gbp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_life.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_life = clean_life[\n",
    "    clean_life.postcode.isin(ukpostcodes.postcode) &\n",
    "    (clean_life.eu_contribution > 0)\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_life.drop(['project_websummary', 'project_no', 'lead_partner_country', 'country', 'themes', \\\n",
    "                 'keywords', 'habitats', 'species', 'base_url', 'area_code', 'latitude', 'longitude' \\\n",
    "                ], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_life.rename(columns={'year_of_finance': 'year'}, inplace=True)\n",
    "output_life.rename(columns={'total_budget': 'total_budget_eur'}, inplace=True)\n",
    "output_life.rename(columns={'eu_contribution': 'eu_contribution_eur'}, inplace=True)\n",
    "output_life.rename(columns={'project_website': 'website'}, inplace=True)\n",
    "output_life.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_life['my_eu_id'] = 'life_' + output_life.index.map(str)\n",
    "output_life.my_eu_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_life.to_pickle('output/life.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
